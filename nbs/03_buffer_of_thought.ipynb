{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: buffer_of_thought.html\n",
    "title: Buffer of Thought\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp agent_coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from langgraph.graph import StateGraph, END, START, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Annotated, Literal, TypedDict, Optional\n",
    "from operator import add, attrgetter\n",
    "import textwrap\n",
    "import os\n",
    "from trustcall import create_extractor\n",
    "import re\n",
    "from pathlib import Path\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "problem_distiller_prompt = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    ## Problem Distiller\n",
    "\n",
    "    You are a highly professional and intelligent expert in information distillation. Your role is to extract essential information from user input queries to solve problems effectively. You also transform this extracted information into a suitable format based on the type of issue.\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Task Instructions\n",
    "\n",
    "    1. **Key Information**:\n",
    "    - Extract values and key variables from the user input.\n",
    "    - Ensure all essential information required to solve the problem is provided.\n",
    "    - Hand over this distilled information to the respective expert for task resolution.\n",
    "\n",
    "    2. **Restrictions**:\n",
    "    - Identify the objective of the problem.\n",
    "    - Outline any corresponding constraints that must be adhered to.\n",
    "\n",
    "    3. **Distilled Task**:\n",
    "    - Extend the problem based on the extracted key information and constraints.\n",
    "    - Summarize a meta problem that addresses the user query and accommodates more input and output variations.\n",
    "    - Incorporate the real-world scenario of the extended problem.\n",
    "    - Define types of key variables and information constraints from the original problem to restrict variables in the extended problem.\n",
    "    - Use the input key information from the user query as an example to solve the problem.\n",
    "    \"\"\"\n",
    ").strip()\n",
    "\n",
    "\n",
    "instantiate_reasoning_prompt = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    ## Meta Reasoner\n",
    "\n",
    "    You are a Meta Reasoner who is extremely knowledgeable in various fields, including Computer Science, Math, Physics, Literature, History, Chemistry, Logical Reasoning, Culture, and Language. You are also skilled in applying high-level reasoning structures for different tasks. \n",
    "\n",
    "    ### Reasoning Structures:\n",
    "\n",
    "    1. **Prompt-based Structure**:\n",
    "    - **Best For**: Common Sense Reasoning, Application Scheduling.\n",
    "    \n",
    "    2. **Procedure-based Structure**:\n",
    "    - **Best For**: Creative tasks like Creative Language Generation, and Text Comprehension.\n",
    "    \n",
    "    3. **Programming-based Structure**:\n",
    "    - **Best For**: Mathematical Reasoning, Code Programming.\n",
    "    - Can transform real-world problems into programming problems to solve efficiently.\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Reasoning Instantiation\n",
    "\n",
    "    **Your Task:**\n",
    "\n",
    "    1. **Contextual Analysis**: Deliberately consider the context and the problem distilled from the problem distiller. Use your understanding to identify a suitable domain expert for solving the problem.\n",
    "\n",
    "    2. **Structure Selection**: Based on the distilled information, select one of the reasoning structures suitable for addressing the problem.\n",
    "\n",
    "    3. **Template Application**: If a thought-template is provided, directly follow it to instantiate the solution for the given problem.\n",
    "    \"\"\"\n",
    ").strip()\n",
    "\n",
    "\n",
    "template_distiller_prompt = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    ## Prompt for Template Distillation:\n",
    "\n",
    "    **User Input**:\n",
    "    **Problem Description**: {distilled_problem}\n",
    "    **Solution Steps or Code**: {solution_steps}\n",
    "\n",
    "    1. **Core task summarization**:\n",
    "    Identify and describe the basic type and core challenges of the problem, such as classifying it as a mathematical problem (e.g., solving a quadratic equation), a data structure problem (e.g., array sorting), an algorithm problem (e.g., search algorithms), etc. And analyze the most efficient way to solve the problem.\n",
    "\n",
    "    2. **Solution Steps Description**:\n",
    "    Outline the general solution steps, including how to define the problem, determine variables, list key equations or constraints, choose appropriate solving strategies and methods, and how to verify the correctness of the results.\n",
    "\n",
    "    3. **General Answer Template**:\n",
    "    Based on the above analysis, propose a template or approach that can be widely applied to this type of problem, including possible variables, functions, class definitions, etc. If it is a programming problem, provide a set of base classes and interfaces that can be used to construct solutions to specific problems.\n",
    "\n",
    "    Please ensure that your response is highly concise and structured, so that specific solutions can be transformed into generalizable methods.\n",
    "\n",
    "    [Optional] Here are some exemplars of the thought-template:\n",
    "\n",
    "    <in-task-examples>\n",
    "    {in_task_examples}\n",
    "    <in-task-examples>\n",
    "\n",
    "    <cross-task-examples>\n",
    "    {cross_task_examples}\n",
    "    <cross-task-examples>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "structure_prompt = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    Extract the items of the 'ThoughtTemplate' Pydantic class from the previous conversation.\n",
    "                                    \n",
    "    <convo>\n",
    "    {conversation}\n",
    "    </convo> \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ThoughtTemplate(BaseModel):\n",
    "    \"\"\"Defining the three fields of the Thought Template\"\"\"\n",
    "    description: str = Field(description=\"Task Description\")\n",
    "    solution: str = Field(description=\"Solution Description\")\n",
    "    template: str = Field(description=\"Thought Template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Setup Large Language Model (LLM)\n",
    "LLM = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Define Trustcall instance for sane extraction of the Thought Template items: Task Description, Solution Description, Thought Template\n",
    "structure_template_text = create_extractor(\n",
    "    LLM, tools=[ThoughtTemplate], tool_choice=\"ThoughtTemplate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Create store with semantic search enabled\n",
    "template_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "        \"dims\": 1536,\n",
    "        \"fields\": ['task_description']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two helper functions to process the memory items by translating a markdown into a JSON following the ThoughtTemplate pattern and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def markdown_to_json(markdown_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Splits a markdown file along its ## headers and organizes content in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        markdown_text: The markdown file content as a string.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are transformed headers and values are paragraph text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Regular expression to match `##` headers\n",
    "    header_pattern = re.compile(r\"^##\\s*(.+)$\", re.MULTILINE)\n",
    "\n",
    "    # Find all headers and their positions\n",
    "    headers = [\n",
    "        (match.start(), match.group(1))\n",
    "        for match in header_pattern.finditer(markdown_text)\n",
    "    ]\n",
    "\n",
    "    # Dictionary to store the processed headers and content\n",
    "    content_dict = {}\n",
    "\n",
    "    for i, (start_pos, header) in enumerate(headers):\n",
    "        # Determine the start and end of the content for this header\n",
    "        end_pos = headers[i + 1][0] if i + \\\n",
    "            1 < len(headers) else len(markdown_text)\n",
    "\n",
    "        # Extract content for the current header\n",
    "        content = markdown_text[start_pos:end_pos]\n",
    "\n",
    "        # Remove the header itself from the content\n",
    "        content = header_pattern.sub(\"\", content, count=1).strip()\n",
    "\n",
    "        # Transform header: casefold, strip whitespace, and replace spaces with underscores\n",
    "        transformed_header = header.casefold().strip().replace(\" \", \"_\")\n",
    "\n",
    "        # Add the header and its corresponding content to the dictionary\n",
    "        content_dict[transformed_header] = content\n",
    "\n",
    "    return content_dict\n",
    "\n",
    "\n",
    "def json_to_markdown(content_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Reconstructs a markdown string from a dictionary where keys are headers\n",
    "    and values are paragraph text.\n",
    "\n",
    "    Args:\n",
    "        content_dict: A dictionary with transformed headers as keys\n",
    "                             and content as values.\n",
    "\n",
    "    Returns:\n",
    "        The reconstructed markdown string.\n",
    "    \"\"\"\n",
    "    markdown_lines = []\n",
    "\n",
    "    for header, content in content_dict.items():\n",
    "        # Transform the key back into a header: replace underscores with spaces, capitalize\n",
    "        original_header = header.replace(\"_\", \" \").title()\n",
    "        # Add the header and its content in markdown format\n",
    "        markdown_lines.append(f\"## {original_header}\")\n",
    "        markdown_lines.append(content)\n",
    "        markdown_lines.append(\"\")  # Add a blank line for spacing\n",
    "\n",
    "    # Join the lines to form the complete markdown text\n",
    "    return \"\\n\".join(markdown_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can populate the storage with some Thought Templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "user_id = \"user_123\"\n",
    "memory = \"thought_templates\"\n",
    "\n",
    "populate = False\n",
    "# Optional, we populate the memory with some templates\n",
    "if populate:\n",
    "    template_path = Path(\"data\") / memory\n",
    "    for md_fl in template_path.iterdir():\n",
    "        template_name, _ = md_fl.name.split(\".\")\n",
    "        with open(md_fl) as fl:\n",
    "            template_store.put(\n",
    "                (user_id, memory), template_name, markdown_to_json(fl.read())\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class BoTState(MessagesState):\n",
    "    template_text: Optional[str]\n",
    "    distilled_problem: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def problem_distiller(state: BoTState) -> dict:\n",
    "    distilled_problem = LLM.invoke(\n",
    "        [problem_distiller_prompt, state['messages'][-1]])\n",
    "    return {\n",
    "        'distilled_problem': distilled_problem.content,\n",
    "        'messages': distilled_problem\n",
    "    }\n",
    "\n",
    "\n",
    "def template_retrieval(state: BoTState, config: RunnableConfig, store: BaseStore) -> dict:\n",
    "    items = store.search(\n",
    "        (user_id, memory), query=state[\"distilled_problem\"], limit=1\n",
    "    )\n",
    "    template = items.pop() if items else None\n",
    "    if template and template.score > config['configurable']['retrieval_threshold']:\n",
    "        template_text = json_to_markdown(\n",
    "            {\n",
    "                'solution_description': template.value['solution_description'],\n",
    "                'thought_template': template.value['thought_template']\n",
    "            }\n",
    "        )\n",
    "        return {'template_text': template_text}\n",
    "    else:\n",
    "        return {'template_text': None}\n",
    "\n",
    "\n",
    "def instantiate_reasoning(state: BoTState, config: RunnableConfig, store: BaseStore) -> dict:\n",
    "    if state['template_text']:\n",
    "        return {\n",
    "            'messages': LLM.invoke([state['distilled_problem'], instantiate_reasoning_prompt + f\"\\n<thought-template>\\n{state['template_text']}\\n<thought-template>\"])\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'messages': LLM.invoke([state['distilled_problem'], instantiate_reasoning_prompt])\n",
    "        }\n",
    "\n",
    "\n",
    "def template_distillation(state: BoTState, config: RunnableConfig, store: BaseStore) -> dict:\n",
    "    items = store.search(\n",
    "        (user_id, memory), query=state[\"messages\"][-1].content, limit=100\n",
    "    )\n",
    "    in_task_l = [template for template in items if template.score >\n",
    "                 config['configurable']['in_task_threshold']]\n",
    "    in_task = in_task_l.pop() if in_task_l else None\n",
    "    cross_task_l = [template for template in items if template.score <= config['configurable']\n",
    "                    ['in_task_threshold'] and template.score > config['configurable']['cross_task_threshold']]\n",
    "    cross_task = cross_task_l.pop() if cross_task_l else None\n",
    "    return {'messages': LLM.invoke(template_distiller_prompt.format(\n",
    "        distilled_problem=state['distilled_problem'],\n",
    "        solution_steps=state['messages'][-1].content,\n",
    "        in_task_examples=json_to_markdown(in_task.value if in_task else {}),\n",
    "        cross_task_examples=json_to_markdown(\n",
    "            cross_task.value if cross_task else {})\n",
    "    ))}\n",
    "\n",
    "\n",
    "def dynamic_meta_buffer_update(state: BoTState, config: RunnableConfig, store: BaseStore) -> dict:\n",
    "    # Invoke the extractor\n",
    "    result = structure_template_text.invoke(\n",
    "        {\"messages\": [structure_prompt.format(state['messages'][-1].content)]})\n",
    "\n",
    "    r, rmeta = result[\"responses\"].pop(), result[\"response_metadata\"].pop()\n",
    "\n",
    "    items = store.search(\n",
    "        (user_id, memory), query=r.task_description, limit=1\n",
    "    )\n",
    "\n",
    "    template = items.pop() if items else None\n",
    "    if template and template.score < config['configurable']['update_threshold']:\n",
    "        store.put((user_id, memory),\n",
    "                  rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n",
    "                  r.model_dump(mode=\"json\"),\n",
    "                  )\n",
    "        return {'messages': 'Meta Buffer updated'}\n",
    "    else:\n",
    "        return {'messages': 'No update'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
