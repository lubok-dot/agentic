{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: download_bot_datasets.html\n",
    "title: Datasets\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp download_bot_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This Notebook collects scripts to download various benchmark-datasets. Most of them can be found on huggingface.\n",
    "\n",
    "| Name                                                                                   | Description                                                                                                                                                                                                                   |\n",
    "|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [GSM8K](https://huggingface.co/datasets/openai/gsm8k)                                  | A dataset of 8,000 high-quality grade-school math problems, used to benchmark models on mathematical reasoning and problem-solving skills.                                                                                     |\n",
    "| [AmbigNQ](https://nlp.cs.washington.edu/ambigqa/)                         | A subset of Natural Questions focusing on ambiguous questions with multiple valid answers, useful for evaluating how well models handle ambiguity in queries.                                                                  |\n",
    "| [HumanEval](https://huggingface.co/datasets/openai/openai_humaneval)                                      | A dataset for evaluating the correctness of Python code generated by models, providing function signatures, docstrings, and test cases to assess the model's programming capabilities.                                          |\n",
    "| [Game of 24](https://github.com/princeton-nlp/tree-of-thought-llm)                            | A mathematical reasoning dataset based on the game where players manipulate four numbers with basic arithmetic operations to reach 24, testing a model's numerical reasoning and calculation skills. It was introduced in the paper *Tree of Thoughts (ToT)*.                          |\n",
    "| [Python Programming Puzzles (P3)](https://github.com/microsoft/PythonProgrammingPuzzles)| A collection of Python programming challenges designed to probe the limits of programming comprehension and reasoning, valuable for evaluating logic, problem-solving, and programming ability. Each puzzle is defined by a Python function that returns True when provided with a correct input, serving as a verifier for candidate solutions.                                |\n",
    "| [BIG-Bench Hard (BBH)](https://github.com/google/BIG-bench)                  | BIG-Bench benchmark focusing on particularly challenging tasks for language models, spanning diverse categories like reasoning, linguistics, and abstract thought.                                               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# load_dataset from the datasets library: Facilitates loading datasets from the Hugging Face Hub. (pip install datasets)\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_file_from_url(\n",
    "    url: str,\n",
    "    path: Path,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download a file from a URL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url\n",
    "        URL of the file to download.\n",
    "    path\n",
    "        Path where the downloaded file will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function performs file download but does not return any value.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        print(f\"File downloaded and saved to {path}.\")\n",
    "    else:\n",
    "        print(f\"File {path} already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_from_huggingface(data_path: Path, path: str, name: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Download a dataset from Hugging Face and save its splits to a specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path\n",
    "        The root directory where the dataset will be saved.\n",
    "    path\n",
    "        The dataset identifier on Hugging Face (e.g., \"openai_humaneval\").\n",
    "    name\n",
    "        An additional name of the dataset. Will be used to create an extra directory. If None, no extra directory is created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally.\n",
    "    \"\"\"\n",
    "    # Construct the target directory\n",
    "    target_path = data_path / path\n",
    "    if name:\n",
    "        target_path = target_path / name\n",
    "\n",
    "    # Check if the target directory exists and contains files\n",
    "    if target_path.exists() and any(target_path.iterdir()):\n",
    "        print(f\"Dataset already exists in {target_path}. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    # Load the dataset from Hugging Face\n",
    "    dataset = load_dataset(path=path, name=name)\n",
    "\n",
    "    # Ensure the target directory exists\n",
    "    target_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save each split of the dataset to the target directory\n",
    "    for split in dataset.keys():\n",
    "        split_dataset = dataset[split]\n",
    "        split_path = target_path / f\"{split}.jsonl\"\n",
    "        split_dataset.to_json(split_path)\n",
    "        print(f\"Saved {split} split to {split_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_gsm8k(\n",
    "    data_path: Path,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download the GSM8k dataset into a specified folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path\n",
    "        Directory path where the GSM8k dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally but does not return any value.\n",
    "    \"\"\"\n",
    "    download_from_huggingface(data_path, \"openai/gsm8k\", \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists in data/openai/gsm8k/main. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_gsm8k(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_ambignq(\n",
    "    path: Path,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download the AmbigNQ dataset into a specified folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path\n",
    "        Directory path where the AmbigNQ dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset, extracts its contents, and saves them locally but does not return any value.\n",
    "    \"\"\"\n",
    "    # URL for AmbigNQ dataset\n",
    "    url = \"https://nlp.cs.washington.edu/ambigqa/data/ambignq_light.zip\"\n",
    "\n",
    "    # Convert path to Path object and create directory if it doesn't exist\n",
    "    ambignq_path = path / \"ambignq\"\n",
    "    ambignq_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download the ZIP file\n",
    "    print(f\"Downloading ZIP file from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Open the ZIP file in memory and extract its contents\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:\n",
    "        print(\"Unpacking ZIP file...\")\n",
    "        zip_file.extractall(ambignq_path)\n",
    "    print(f\"Unpacked contents to {ambignq_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ZIP file from https://nlp.cs.washington.edu/ambigqa/data/ambignq_light.zip...\n",
      "Unpacking ZIP file...\n",
      "Unpacked contents to data/ambignq\n"
     ]
    }
   ],
   "source": [
    "download_ambignq(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_humaneval(data_path: Path):\n",
    "    \"\"\"\n",
    "    Download the HumanEval dataset and save it to the specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path\n",
    "        The directory path where the HumanEval dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally but does not return any value.\n",
    "    \"\"\"\n",
    "    download_from_huggingface(data_path, path=\"openai_humaneval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists in data/openai_humaneval. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_humaneval(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_game_of_24(data_path: Path):\n",
    "    \"\"\"\n",
    "    Download the Game of 24 dataset using Hugging Face's `load_dataset` and save it to the specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path\n",
    "        The directory path where the Game of 24 dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally but does not return any value.\n",
    "    \"\"\"\n",
    "    download_from_huggingface(data_path, path=\"nlile/24-game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists in data/nlile/24-game. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_game_of_24(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_python_programming_puzzles(path: Path):\n",
    "    \"\"\"\n",
    "    Download the Python Programming Puzzles (P3) dataset and save it to the specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path\n",
    "        The directory path where the P3 dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally but does not return any value.\n",
    "    \"\"\"\n",
    "    # Ensure the target directory exists\n",
    "    p3_path = path / \"python_programming_puzzles\"\n",
    "    p3_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # URL for the P3 dataset (update this URL if needed)\n",
    "    url = \"https://raw.githubusercontent.com/microsoft/PythonProgrammingPuzzles/main/puzzles/puzzles.json\"\n",
    "\n",
    "    download_file_from_url(url, p3_path / \"train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/python_programming_puzzles/train.json already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_python_programming_puzzles(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_bhb_tasks(data_path: Path):\n",
    "    \"\"\"\n",
    "    Download specific tasks from Google's BIG-Bench Hard (BBH) dataset and save them to the specified directory.\n",
    "    Instead of downloading the entire dataset from the official repository, we download some examples from huggingface.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : Path\n",
    "        The root directory where the BBH tasks will be saved. Each task will be stored in a subdirectory\n",
    "        based on its respective name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the BBH tasks and saves them locally, with separate directories for each task.\n",
    "    \"\"\"\n",
    "    for path, name in [\n",
    "        (\"maveriq/bigbenchhard\", \"boolean_expressions\"),\n",
    "        (\"maveriq/bigbenchhard\", \"causal_judgement\"),\n",
    "        (\"maveriq/bigbenchhard\", \"date_understanding\"),\n",
    "        (\"maveriq/bigbenchhard\", \"word_sorting\"),\n",
    "    ]:\n",
    "        download_from_huggingface(data_path, path, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists in data/maveriq/bigbenchhard/boolean_expressions. Skipping download.\n",
      "Dataset already exists in data/maveriq/bigbenchhard/causal_judgement. Skipping download.\n",
      "Dataset already exists in data/maveriq/bigbenchhard/date_understanding. Skipping download.\n",
      "Dataset already exists in data/maveriq/bigbenchhard/word_sorting. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_bhb_tasks(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def sample_from_datasets(\n",
    "    N: int, data_path: Path, path: Optional[str] = None, name: Optional[str] = None\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Randomly samples N instances from the dataset(s) stored at the specified location.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    N :\n",
    "        Number of samples to retrieve.\n",
    "    data_path :\n",
    "        Root directory containing the datasets.\n",
    "    path :\n",
    "        Path to the specific dataset within the root directory. If None, `name` must also be None.\n",
    "    name :\n",
    "        Name of the dataset ('train' or 'test'). If None, samples from both datasets in `data_path/path`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    samples :\n",
    "        A list of sampled instances.\n",
    "    \"\"\"\n",
    "    if path is None and name is not None:\n",
    "        raise ValueError(\"If 'path' is None, 'name' must also be None.\")\n",
    "\n",
    "    target_path = data_path / path if path else data_path\n",
    "    target_path = target_path / name if name else target_path\n",
    "\n",
    "    # Collect all json/jsonl files to sample from\n",
    "    if target_path.exists():\n",
    "        dataset_files = [\n",
    "            file\n",
    "            for file in target_path.rglob(\"*\")\n",
    "            if file.suffix in [\".json\", \".jsonl\"]\n",
    "        ]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"The path '{target_path}' does not exist.\")\n",
    "\n",
    "    if not dataset_files:\n",
    "        raise ValueError(\n",
    "            f\"No JSON or JSONL files found in the path '{target_path}'.\")\n",
    "\n",
    "    # Load data from the dataset files\n",
    "    all_data = []\n",
    "    for dataset_file in dataset_files:\n",
    "        with open(dataset_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            if dataset_file.suffix == \".jsonl\":  # JSONL: Read line by line\n",
    "                data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "            elif dataset_file.suffix == \".json\":  # JSON: Use json.load()\n",
    "                data = json.load(f)\n",
    "                if not isinstance(data, list):\n",
    "                    raise ValueError(\n",
    "                        f\"The JSON file '{\n",
    "                            dataset_file}' does not contain a list of records.\"\n",
    "                    )\n",
    "            else:\n",
    "                continue  # Skip unsupported file types\n",
    "            all_data.extend(data)\n",
    "\n",
    "    if not all_data:\n",
    "        raise ValueError(\"No data found in the specified datasets.\")\n",
    "\n",
    "    # Randomly sample N instances\n",
    "    if len(all_data) < N:\n",
    "        raise ValueError(\n",
    "            f\"Not enough data to sample {\n",
    "                N} instances. Found only {len(all_data)} records.\"\n",
    "        )\n",
    "\n",
    "    return random.sample(all_data, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': '( True and not True and True ) is', 'target': 'False'}, {'input': '( ( not False ) and False ) is', 'target': 'False'}, {'input': '( False ) and ( not False ) is', 'target': 'False'}, {'input': 'not False or ( False ) or True is', 'target': 'True'}, {'input': 'False and ( False or not False ) is', 'target': 'False'}, {'input': 'True and not False or False and False is', 'target': 'True'}, {'input': 'True or ( ( not True ) ) is', 'target': 'True'}, {'input': 'False or ( not False and False ) is', 'target': 'False'}, {'input': '( ( True ) ) or not False is', 'target': 'True'}, {'input': 'not not not False and True and False is', 'target': 'False'}]\n",
      "[{'annotations': [{'type': 'singleAnswer', 'answer': ['Cristiano Ronaldo']}], 'id': '31162199496959700', 'question': 'Who is the person with most instagram followers?'}, {'annotations': [{'type': 'multipleQAs', 'qaPairs': [{'question': 'When was sound captured for the first time but not able to be played back?', 'answer': ['1857']}, {'question': 'When was sound captured and able to be played back for the first time?', 'answer': ['1877']}]}], 'id': '706472480901161359', 'question': 'When was sound captured for the first time?'}, {'numbers': [2, 5, 6, 6], 'solutions': ['(5×2-6)×6', '(5-2)×6+6'], 'solvable': True, 'amt': 11.91, 'solved_rate': 0.782, 'mean_time': 10.44, 'std_time': 6.03}, {'annotations': [{'type': 'multipleQAs', 'qaPairs': [{'question': \"Who is Elizabeth's primary love interest in Pirates of the Caribbean?\", 'answer': ['Will Turner']}, {'question': 'Who does Elizabeth develop feelings for after being marooned with him in Pirates of the Caribbean?', 'answer': ['Jack Sparrow']}]}], 'id': '1272180233249547778', 'question': 'Who does elizabeth love in pirates of the caribbean?'}, {'annotations': [{'type': 'singleAnswer', 'answer': ['Saint Paul', 'Paul the Apostle', 'Paul']}, {'type': 'singleAnswer', 'answer': ['Saul of Tarsus', 'Saint Paul', 'Paul the Apostle']}], 'id': '7967363756869292425', 'question': 'Who came up with the rule no work no eat?'}]\n"
     ]
    }
   ],
   "source": [
    "# Sample 10 instances from a specific dataset\n",
    "samples = sample_from_datasets(\n",
    "    N=10, data_path=data_path, path=\"maveriq/bigbenchhard\", name=\"boolean_expressions\"\n",
    ")\n",
    "print(samples)\n",
    "\n",
    "# Sample 5 instances randomly from all datasets\n",
    "samples = sample_from_datasets(N=5, data_path=data_path)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
