{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: download_bot_datasets.html\n",
    "title: Datasets\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp download_bot_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This Notebook collects scripts to download various benchmark-datasets. Most of them can be found on huggingface.\n",
    "\n",
    "| Name                                                                                   | Description                                                                                                                                                                                                                   |\n",
    "|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| [GSM8K](https://huggingface.co/datasets/openai/gsm8k)                                  | A dataset of 8,000 high-quality grade-school math problems, used to benchmark models on mathematical reasoning and problem-solving skills.                                                                                     |\n",
    "| [AmbigNQ](https://nlp.cs.washington.edu/ambigqa/)                         | A subset of Natural Questions focusing on ambiguous questions with multiple valid answers, useful for evaluating how well models handle ambiguity in queries.                                                                  |\n",
    "| [HumanEval](https://huggingface.co/datasets/openai/openai_humaneval)                                      | A dataset for evaluating the correctness of Python code generated by models, providing function signatures, docstrings, and test cases to assess the model's programming capabilities.                                          |\n",
    "| [Game of 24](https://github.com/princeton-nlp/tree-of-thought-llm)                            | A mathematical reasoning dataset based on the game where players manipulate four numbers with basic arithmetic operations to reach 24, testing a model's numerical reasoning and calculation skills.                            |\n",
    "| [Python Programming Puzzles (P3)](https://github.com/microsoft/PythonProgrammingPuzzles)| A collection of Python programming challenges designed to probe the limits of programming comprehension and reasoning, valuable for evaluating logic, problem-solving, and programming ability.                                  |\n",
    "| [BIG-Bench Hard (BBH)](https://github.com/google/BIG-bench)                  | A subset of the BIG-Bench benchmark focusing on particularly challenging tasks for language models, spanning diverse categories like reasoning, linguistics, and abstract thought.                                               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverpfante/Documents/agentic/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "# load_dataset from the datasets library: Facilitates loading datasets from the Hugging Face Hub. (pip install datasets)\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def download_file_from_url(\n",
    "    url: str,\n",
    "    filepath: Path,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download a file from a URL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url\n",
    "        URL of the file to download.\n",
    "    filepath\n",
    "        Path where the downloaded file will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function performs file download but does not return any value.\n",
    "    \"\"\"\n",
    "    if not filepath.exists():\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        print(f\"File downloaded and saved to {filepath}.\")\n",
    "    else:\n",
    "        print(f\"File {filepath} already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def download_from_huggingface(data_path: Path, path: str, name: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Download a dataset from Hugging Face and save its splits to a specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : Path\n",
    "        The root directory where the dataset will be saved.\n",
    "    path : str\n",
    "        The dataset identifier on Hugging Face (e.g., \"openai_humaneval\").\n",
    "    name :\n",
    "        An additional name of the dataset. Will be used to create an extra directory. If None, no extra directory is created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally.\n",
    "    \"\"\"\n",
    "    # Construct the target directory\n",
    "    target_path = data_path / path\n",
    "    if name:\n",
    "        target_path = target_path / name\n",
    "\n",
    "    # Check if the target directory exists and contains files\n",
    "    if target_path.exists() and any(target_path.iterdir()):\n",
    "        print(f\"Dataset already exists in {target_path}. Skipping download.\")\n",
    "        return\n",
    "\n",
    "    # Load the dataset from Hugging Face\n",
    "    dataset = load_dataset(path=path, name=name)\n",
    "\n",
    "    # Ensure the target directory exists\n",
    "    target_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save each split of the dataset to the target directory\n",
    "    for split in dataset.keys():\n",
    "        split_dataset = dataset[split]\n",
    "        split_path = target_path / f\"{split}.jsonl\"\n",
    "        split_dataset.to_json(split_path)\n",
    "        print(f\"Saved {split} split to {split_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def download_gsm8k(\n",
    "    data_path: Path,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download the GSM8k dataset into a specified folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path\n",
    "        Directory path where the GSM8k dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally but does not return any value.\n",
    "    \"\"\"\n",
    "    download_from_huggingface(data_path, \"openai/gsm8k\", \"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists in data/openai/gsm8k/main. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_gsm8k(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def download_ambignq(\n",
    "    path: Path,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download the AmbigNQ dataset into a specified folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path\n",
    "        Directory path where the AmbigNQ dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset, extracts its contents, and saves them locally but does not return any value.\n",
    "    \"\"\"\n",
    "    # URL for AmbigNQ dataset\n",
    "    url = \"https://nlp.cs.washington.edu/ambigqa/data/ambignq_light.zip\"\n",
    "\n",
    "    # Convert path to Path object and create directory if it doesn't exist\n",
    "    ambignq_path = path / \"ambignq\"\n",
    "    ambignq_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download the ZIP file\n",
    "    print(f\"Downloading ZIP file from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Open the ZIP file in memory and extract its contents\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:\n",
    "        print(\"Unpacking ZIP file...\")\n",
    "        zip_file.extractall(ambignq_path)\n",
    "    print(f\"Unpacked contents to {ambignq_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ZIP file from https://nlp.cs.washington.edu/ambigqa/data/ambignq_light.zip...\n",
      "Unpacking ZIP file...\n",
      "Unpacked contents to data/ambignq\n"
     ]
    }
   ],
   "source": [
    "download_ambignq(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def download_humaneval(data_path: Path):\n",
    "    \"\"\"\n",
    "    Download the HumanEval dataset and save it to the specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path :\n",
    "        The directory path where the HumanEval dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally but does not return any value.\n",
    "    \"\"\"\n",
    "    download_from_huggingface(data_path, path=\"openai_humaneval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists in data/openai_humaneval. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_humaneval(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def download_game_of_24(data_path: Path):\n",
    "    \"\"\"\n",
    "    Download the Game of 24 dataset using Hugging Face's `load_dataset` and save it to the specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path\n",
    "        The directory path where the Game of 24 dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally but does not return any value.\n",
    "    \"\"\"\n",
    "    download_from_huggingface(data_path, path=\"nlile/24-game\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists in data/nlile/24-game. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_game_of_24(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def download_python_programming_puzzles(path: Path):\n",
    "    \"\"\"\n",
    "    Download the Python Programming Puzzles (P3) dataset and save it to the specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path\n",
    "        The directory path where the P3 dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally but does not return any value.\n",
    "    \"\"\"\n",
    "    # Ensure the target directory exists\n",
    "    p3_path = path / \"python_programming_puzzles\"\n",
    "    p3_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # URL for the P3 dataset (update this URL if needed)\n",
    "    url = \"https://github.com/microsoft/PythonProgrammingPuzzles/blob/main/puzzles/puzzles.json\"\n",
    "\n",
    "    download_file_from_url(url, p3_path / 'puzzles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/python_programming_puzzles/puzzles.json already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_python_programming_puzzles(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def download_bhb_tasks(data_path: Path):\n",
    "    \"\"\"\n",
    "    Download specific tasks from Google's BIG-Bench Hard (BBH) dataset and save them to the specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : Path\n",
    "        The root directory where the BBH tasks will be saved. Each task will be stored in a subdirectory \n",
    "        based on its respective name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the BBH tasks and saves them locally, with separate directories for each task.\n",
    "    \"\"\"\n",
    "    for path, name in [\n",
    "        (\"maveriq/bigbenchhard\", \"boolean_expressions\"),\n",
    "        (\"maveriq/bigbenchhard\", \"causal_judgement\"),\n",
    "        (\"maveriq/bigbenchhard\", \"date_understanding\"),\n",
    "        (\"maveriq/bigbenchhard\", \"word_sorting\")\n",
    "    ]:\n",
    "        download_from_huggingface(data_path, path, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists in data/maveriq/bigbenchhard/boolean_expressions. Skipping download.\n",
      "Dataset already exists in data/maveriq/bigbenchhard/causal_judgement. Skipping download.\n",
      "Dataset already exists in data/maveriq/bigbenchhard/date_understanding. Skipping download.\n",
      "Dataset already exists in data/maveriq/bigbenchhard/word_sorting. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_bhb_tasks(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
