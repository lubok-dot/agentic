{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus our analysis on three datasets from papers and provide a script to download each.\n",
    "\n",
    "| **Dataset** | **Tool Used** | **Dataset Explanation** |\n",
    "|-------------|---------------|-------------------------|\n",
    "| **GSM8k**   | Code interpreter for mathematical reasoning | **Purpose:** Mathematical word problem-solving for arithmetic reasoning.<br>**Size:** 8,000 questions.<br>**Download:** Available at the [GSM8k GitHub repository](https://github.com/openai/grade-school-math). |\n",
    "| **AmbigNQ** | Wikipedia search and Google for fact-checking | **Purpose:** Open-domain QA for ambiguous questions with multiple possible answers.<br>**Size:** ~14,042 examples.<br>**Download:** Accessible from the [Google NQ dataset page](https://ai.google.com/research/NaturalQuestions). |\n",
    "| **HumanEval** | Code interpreter for code generation evaluation | **Purpose:** Evaluating functional correctness in code generation from docstrings.<br>**Size:** 164 programming problems.<br>**Download:** Available via the [Hugging Face Datasets library](https://huggingface.co/datasets/openai/openai_humaneval). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/oliverpfante/Documents/agentic/nbs/data')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "\n",
    "path = Path.cwd().parent / \"data\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_file(\n",
    "    url: str,\n",
    "    filepath: Path,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download a file from a URL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url\n",
    "        URL of the file to download.\n",
    "    filepath\n",
    "        Path where the downloaded file will be saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function performs file download but does not return any value.\n",
    "    \"\"\"\n",
    "    if not filepath.exists():\n",
    "        print(f\"Downloading file from {url}...\")\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        print(f\"File downloaded and saved to {filepath}.\")\n",
    "    else:\n",
    "        print(f\"File {filepath} already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_gsm8k(\n",
    "    path: Path,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download the GSM8k dataset into a specified folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path\n",
    "        Directory path where the GSM8k dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally but does not return any value.\n",
    "    \"\"\"\n",
    "    # URLs for GSM8k dataset\n",
    "    train_url = \"https://raw.githubusercontent.com/openai/grade-school-math/refs/heads/master/grade_school_math/data/train.jsonl\"\n",
    "    test_url = \"https://raw.githubusercontent.com/openai/grade-school-math/refs/heads/master/grade_school_math/data/test.jsonl\"\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    gsm8k_path = path / \"gsm8k\"\n",
    "    gsm8k_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download files\n",
    "    download_file(train_url, gsm8k_path / \"gsm8k_train.jsonl\")\n",
    "    download_file(test_url, gsm8k_path / \"gsm8k_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/oliverpfante/Documents/agentic/nbs/data/gsm8k/gsm8k_train.jsonl already exists. Skipping download.\n",
      "File /Users/oliverpfante/Documents/agentic/nbs/data/gsm8k/gsm8k_test.jsonl already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "download_gsm8k(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_ambignq(\n",
    "    path: Path,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download the AmbigNQ dataset into a specified folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path\n",
    "        Directory path where the AmbigNQ dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset, extracts its contents, and saves them locally but does not return any value.\n",
    "    \"\"\"\n",
    "    # URL for AmbigNQ dataset\n",
    "    url = \"https://nlp.cs.washington.edu/ambigqa/data/ambignq_light.zip\"\n",
    "\n",
    "    # Convert path to Path object and create directory if it doesn't exist\n",
    "    ambignq_path = path / \"ambignq\"\n",
    "    ambignq_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download the ZIP file\n",
    "    print(f\"Downloading ZIP file from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Open the ZIP file in memory and extract its contents\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:\n",
    "        print(\"Unpacking ZIP file...\")\n",
    "        zip_file.extractall(ambignq_path)\n",
    "    print(f\"Unpacked contents to {ambignq_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ZIP file from https://nlp.cs.washington.edu/ambigqa/data/ambignq_light.zip...\n",
      "Unpacking ZIP file...\n",
      "Unpacked contents to /Users/oliverpfante/Documents/agentic/nbs/data/ambignq\n"
     ]
    }
   ],
   "source": [
    "download_ambignq(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverpfante/Documents/agentic/langgraph/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "# load_dataset from the datasets library: Facilitates loading datasets from the Hugging Face Hub. (pip install datasets)\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def download_humaneval(path: Path):\n",
    "    \"\"\"\n",
    "    Download the HumanEval dataset and save it to the specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path :\n",
    "        The directory path where the HumanEval dataset will be saved. If the directory does not exist, it will be created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function downloads the dataset and saves it locally but does not return any value.\n",
    "    \"\"\"\n",
    "    # Ensure the target directory exists\n",
    "    humaneval_path = path / \"humaneval\"\n",
    "    humaneval_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load the HumanEval dataset\n",
    "    dataset = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "    # Save each split of the dataset to the specified directory\n",
    "    for split in dataset.keys():\n",
    "        split_dataset = dataset[split]\n",
    "        split_path = humaneval_path / f\"{split}.jsonl\"\n",
    "        split_dataset.to_json(split_path)\n",
    "        print(f\"Saved {split} split to {split_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 105.94ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test split to /Users/oliverpfante/Documents/agentic/nbs/data/humaneval/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "download_humaneval(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 'HumanEval/0',\n",
       " 'prompt': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n',\n",
       " 'canonical_solution': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n',\n",
       " 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\",\n",
       " 'entry_point': 'has_close_elements'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | hide\n",
    "dataset = load_dataset('json', data_files=str(path/'humaneval'/'test.jsonl'))\n",
    "dataset['train'][0][canonical_solution']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
