# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_agent_coder.ipynb.

# %% auto 0
__all__ = ['human_eval_example', 'coder_prompt', 'response', 'structure_code', 'tester_prompt', 'tester_response',
           'structure_test', 'LLM', 'coder_template', 'code', 'tester_template', 'tests', 'workflow', 'memory', 'coder',
           'Coder', 'Tester', 'InputState', 'OverallState', 'write_program', 'get_code', 'write_tests', 'executor',
           'correct_implementation']

# %% ../nbs/01_agent_coder.ipynb 4
from langgraph.graph import StateGraph, END, START, MessagesState
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    PromptTemplate,
)
from langchain_core.output_parsers.string import StrOutputParser
from pydantic import BaseModel, Field
from typing import List, Annotated, Literal, TypedDict
from operator import add, attrgetter
import textwrap
import os
from trustcall import create_extractor

# %% ../nbs/01_agent_coder.ipynb 6
# first example from the HumanEval dataset. Used as one-shot example.

human_eval_example = {
    "code_snippet": 'from typing import List\n\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    """ Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    """\n',
    "entry_point": "has_close_elements",
}

# %% ../nbs/01_agent_coder.ipynb 7
# Programmer Prompt
coder_prompt = textwrap.dedent(
    """
    **Role**: You are a software programmer. 
    
    **Task**: As a programmer, you are required to complete the function. Use a Chain-of-Thought approach to break down the problem, create pseudocode, and then write the code in Python language. Ensure that your code is efficient, readable, and well-commented. For example: 
    
    **Input Code Snippet**: 
    ```python 
    {code_snippet}
    # TODO: Implement the logic to determine if any two numbers are closer than the threshold pass 

    # Add your code here to complete the function
    ```
    **Instructions**: 
    1. **Understand and Clarify**: Make sure you understand the task. 
    2. **Algorithm/Method Selection**: Decide on the most efficient way. 
    3. **Pseudocode Creation**: Write down the steps you will follow in pseudocode. 
    4. **Code Generation**: Translate your pseudocode into executable Python code.
    """
).strip()

# one-shot example
response = textwrap.dedent(
    '''
    As a software developer, I'll follow the Chain-of-Thought approach outlined in the prompt to complete the `has_close_elements` function. Let's break it down into the four steps: 
    
    1. **Understand and Clarify**: 
    - The task is to write a function that checks if any two numbers in a given list are closer to each other than a specified threshold. 
    - The function should return `True` if such a pair exists, otherwise `False`. 
    
    2. **Algorithm/Method Selection**: 
    - A straightforward way to solve this would be to compare each number with every other number in the list. 
    - We can use a nested loop where each number is compared to every number that comes after it in the list. 
    - The absolute difference between each pair will be calculated and compared to the threshold. 
    
    3. **Pseudocode Creation**: 
    ```
    function has_close_elements(numbers, threshold): 
        loop through each number in the list (i) 
            for each number, loop through the remaining numbers in the list (j) 
                if the absolute difference between numbers[i] and numbers[j] is less than or equal to the threshold 
                    return True 
    if no such pair is found, return False 
    ``` 
    
    4. **Code Generation**: 
    - Translate the pseudocode into Python code. Only implement the function. DO NOT add any test cases or explanations. 
    Return the python implementation of the function within triple quotes. Here's a complete example: 
    ```python 
    from typing import List 
    
    def has_close_elements(numbers: List[float], threshold: float) -> bool: 
        """ 
        Check if in given list of numbers, are any two numbers closer to each other than given threshold. 
        
        >>> has_close_elements([1.0, 2.0, 3.0], 0.5) 
        False 
        >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) 
        True 
        """ 
        # Loop through each number in the list 
        for i in range(len(numbers)): 
            # For each number, loop through the remaining numbers 
            for j in range(i + 1, len(numbers)): 
                # If the absolute difference is less than or equal to the threshold, return True 
                if abs(numbers[i] - numbers[j]) <= threshold: 
                    return True 
                    
        # If no such pair is found, return False 
        return False 
        ```
'''
).strip()

# Prompt to structure the output generated by the Programmer
structure_code = textwrap.dedent(
    '''
    Extract or update the items of the 'Coder' Pydantic class from the previous conversation. 
                                    
    <convo>
    {conversation}
    </convo>"""
    '''
).strip()

# %% ../nbs/01_agent_coder.ipynb 8
# Test Designer Prompt
tester_prompt = textwrap.dedent(
    """
    **Role**: As a tester, your task is to create comprehensive test cases for the incomplete `{entry_point}` function. These test cases should encompass Basic, Edge, and Large Scale scenarios to ensure the code's robustness, reliability, and scalability. 
    
    **Input Code Snippet**: 
    ```python
    {code_snippet} 
    ```
    **1. Basic Test Cases**: 
    - **Objective**: To verify the fundamental functionality of the `{entry_point}` function under normal conditions. 
    
    **2. Edge Test Cases**: 
    - **Objective**: To evaluate the function's behavior under extreme or unusual conditions. 
    
    **3. Large Scale Test Cases**: 
    - **Objective**: To assess the function’s performance and scalability with large data samples. 
    
    **Instructions**: 
    - Implement a comprehensive set of test cases following the guidelines above. 
    - Ensure each test case is well-documented with comments explaining the scenario it covers. 
    - Pay special attention to edge cases as they often reveal hidden bugs. 
    - For large-scale tests, focus on the function's efficiency and performance under heavy loads.
    """
).strip()

# one-shot example
tester_response = textwrap.dedent(
    """
    **Role**: As a tester, your task is to create and execute a series of test cases for the `{entry_point}` function. These test cases should include Basic, Edge, and Large Scale scenarios to ensure the function's robustness, reliability, and scalability.

    **Input Code Snippet**:
    ```python
    {code_snippet}
    ```

    **1. Basic Test Cases**:
    - **Objective**: Verify the fundamental functionality of the `{entry_point}` function under normal conditions.
    - **Examples**:
        ```python
        # Test 1: Standard case with close elements
        assert (x := {entry_point}([1.0, 2.5, 3.5, 5.0], 1.0)) == True, str(x) + ' differs from the expected output'
        
        # Test 2: Standard case with no close elements
        assert (x := {entry_point}([1.0, 3.0, 5.0, 7.0], 1.5)) == False, str(x) + ' differs from the expected output'
        ```

    **2. Edge Test Cases**:
    - **Objective**: Evaluate the function's behavior under extreme or unusual conditions.
    - **Examples**:
        ```python
        # Test 1: Empty list
        assert (x := {entry_point}([], 1.0)) == False, str(x) + ' differs from the expected output'
        
        # Test 2: List with all identical elements
        assert (x := {entry_point}([3.0, 3.0, 3.0], 0.0)) == True, str(x) + ' differs from the expected output'
        
        # Test 3: Very small threshold
        assert (x := {entry_point}([1.0, 1.01, 2.0], 0.005)) == False, str(x) + ' differs from the expected output'
        
        # Test 4: List with only two elements
        assert (x := {entry_point}([1.0, 2.0], 1.5)) == True, str(x) + ' differs from the expected output'
        ```

    **3. Large Scale Test Cases**:
    - **Objective**: Assess the function’s performance and scalability with large data samples.
    - **Examples**:
        ```python
        # Test 1: Large list with predictable pattern
        large_list = [i * 0.1 for i in range(100000)]  # Creates a list [0, 0.1, 0.2, ..., 9999.9]
        
        # Test with a threshold where no close elements exist
        assert (x := {entry_point}(large_list, 0.05)) == False, str(x) + ' differs from the expected output'
        
        # Test with a larger threshold where adjacent elements are within the threshold
        assert (x := {entry_point}(large_list, 0.15)) == True, str(x) + ' differs from the expected output'
        ```

    **Instructions**:
    - Implement and execute these test cases.
    - Document any errors, inefficiencies, or unexpected behaviors observed during testing.
    """
).strip()

# Prompt to structure the output of the Test Designer
structure_test = textwrap.dedent(
    '''
    Extract the items of the 'Tester' Pydantic class from the previous conversation. Each item is a list of test cases:
    basic, edge and large scale test cases.
                                    
    <convo>
    {conversation}
    </convo>"""
    '''
).strip()

# %% ../nbs/01_agent_coder.ipynb 10
class Coder(BaseModel):
    """
    Pydantic class where each attribute corresponds to the step the Programmer should
    follow to solve the coding challenge.
    """

    clarify: str = Field(description="Content of the 'Understand and Clarify' section")
    algo: str = Field(description="Content of the 'Algorithm/Method Selection' section")
    pseudocode: str = Field(description="Content of the 'Pseudocode Creation' section")
    code: str = Field(
        description="Content of the 'Code Generation' section. Put only the python code between the triple quotes in this field."
    )


class Tester(BaseModel):
    """
    Pydantic class collecting the basic, edge cases, and large scale test cases.
    """

    basic: List[str] = Field(
        description="List of basic test cases, each of them in Python code. Each item of the list should be a single test, i.e., single assert statement."
    )
    edge: List[str] = Field(
        description="List of edge case test cases, each of them in Python code. Each item of the list should be a single test, i.e., single assert statement."
    )
    large: List[str] = Field(
        description="List of large scale test cases, testing for performance, each of the in Python code. Each item of the list should be a single test, i.e., single assert statement."
    )

# %% ../nbs/01_agent_coder.ipynb 12
# Setup Large Language Model (LLM)
LLM = ChatOpenAI(
    model_name="gpt-4o-mini",
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    temperature=0.0,
)

# %% ../nbs/01_agent_coder.ipynb 14
coder_template = ChatPromptTemplate(
    messages=[
        ("user", coder_prompt.format(**human_eval_example)),
        ("ai", response),
        ("user", coder_prompt),
        MessagesPlaceholder("messages", optional=True),
    ],
    input_variables=["code_snippet", "messages"],
)

# Trustcall object for structuring the Programmer's output
code = create_extractor(LLM, tools=[Coder], tool_choice="Coder", enable_inserts=True)

# %% ../nbs/01_agent_coder.ipynb 15
# Tester's template including the instruction and a one-shot example.
tester_template = ChatPromptTemplate(
    messages=[
        ("user", tester_prompt.format(**human_eval_example)),
        ("ai", tester_response.format(**human_eval_example)),
        ("user", tester_prompt),
    ],
    input_variables=["code_snippet", "entry_point"],
)

# Trustcall object for structuring the Tester's output
tests = create_extractor(LLM, tools=[Tester], tool_choice="Tester")

# %% ../nbs/01_agent_coder.ipynb 17
class InputState(TypedDict):
    code_snippet: (
        str  # function header including its doc-string, i.e., the input from HumanEval
    )
    entry_point: str  # name of the function


class OverallState(InputState, MessagesState):
    # Since we inherit from Message state, we have also a 'messages' channel storing the chat history
    tester: Tester  # test cases, basic, edge cases, and large scale test cases
    # accumulates all implementation attempts
    coder: Annotated[List[Coder], add]
    num_iterations: Annotated[int, add]  # record the number of iterations

# %% ../nbs/01_agent_coder.ipynb 19
def write_program(state: OverallState) -> OverallState:
    """
    Writes the program as string. Represents the 'Programmer'.

    Args:
        state: The current state of the program, which includes
            the input context and the current implementation (if it exists)

    Returns:
        The updated state with new messages containing the generated program.
    """
    return {"messages": (coder_template | LLM).invoke(state), "num_iterations": 1}


def get_code(state: OverallState) -> OverallState:
    """
    Extracts the steps, the Programmer followed to solve the coding challenge as JSON schema
    using a Trustcall runnable. It does it, by updating an already existing schema if it exists.

    Args:
        state: The current state of the program, containing the conversation
            history and the coder's existing implementation as string.

    Returns:
        The updated state with the structured output containing the code in the "coder" key.
    """
    if state["coder"]:
        structured = code.invoke(
            {
                "messages": [
                    (
                        "user",
                        structure_code.format(
                            conversation=state["messages"][-1].content
                        ),
                    )
                ],
                "existing": {"Coder": state["coder"][-1].model_dump()},
            }
        )
    else:
        structured = code.invoke(
            structure_code.format(conversation=state["messages"][-1].content)
        )
    return {"coder": structured["responses"]}


def write_tests(state: OverallState) -> OverallState:
    """
    Generates test cases for the provided state using the defined templates and parsers.
    Represents the 'Test Designer'

    Args:
        state: The current state containing the context and requirements for generating tests.

    Returns:
        An updated state with the generated test cases stored under the "tester" key.
    """
    structured_tests = (
        tester_template
        | LLM
        | StrOutputParser()
        | PromptTemplate.from_template(structure_test)
        | tests
    ).invoke(state)
    return {"tester": structured_tests["responses"][0]}


def executor(state: OverallState) -> OverallState:
    """
    Executes the generated code against the provided test cases and captures error messages.
    Appends them to the message history. This node represents the 'Executor'.

    Args:
        state: The current state containing the code and test cases.

    Returns:
        Updated state with error messages for failed tests, if any.
    """
    error_messages = []
    for test_type in ["basic", "edge", "large"]:
        for test in attrgetter(test_type)(state["tester"]):
            try:
                exec(state["coder"][-1].code + test)
            except Exception as e:
                error_messages.append(
                    (
                        "user",
                        textwrap.dedent(
                            """
                            FAILED TEST

                            Your solution failed the test 
                                    
                            <test>               
                            {test}
                            <test>

                            with the error message:

                            <error>
                            {error}
                            <error>                
                            """
                        )
                        .strip()
                        .format(test=test, error=e),
                    )
                )
    return {"messages": error_messages}

# %% ../nbs/01_agent_coder.ipynb 21
def correct_implementation(state: OverallState) -> Literal["Programmer", "END"]:
    """
    Decides the next workflow step based on test results.

    If any test fails and the maximum number of iterations has not been reached, the
    workflow returns to "Programmer" for refinement. Otherwise, it proceeds to "END".

    Args:
        state: The current workflow state with messages and test results.

    Returns:
        "Programmer" if tests failed, or "END" if all tests passed.
    """
    MAX_NUM_ITERATIONS = 3
    if (
        "FAILED TEST" in state["messages"][-1].content
        and state["num_iterations"] <= MAX_NUM_ITERATIONS
    ):
        return "Programmer"
    else:
        return "END"

# %% ../nbs/01_agent_coder.ipynb 23
workflow = StateGraph(OverallState, input=InputState)

# add nodes
workflow.add_node("Programmer", write_program)
workflow.add_node("Get Code", get_code)
workflow.add_node("Test Designer", write_tests)
workflow.add_node("Executor", executor)

# add edges
workflow.add_edge(START, "Programmer")
workflow.add_edge(START, "Test Designer")
workflow.add_edge("Programmer", "Get Code")
workflow.add_edge("Get Code", "Executor")
workflow.add_edge("Test Designer", "Executor")
workflow.add_conditional_edges(
    "Executor", correct_implementation, {"Programmer": "Programmer", "END": END}
)

# compile the graph

memory = MemorySaver()
coder = workflow.compile(checkpointer=memory)
